\chapter{Implementation}
\textit{In this chapter, we address the implementation phase of the project. After establishing a detailed analysis and a clear design, the implementation represents the concrete culmination of our work, through the deployment of the various components of the project. We will detail here the main steps, the tools used, the challenges encountered, as well as the solutions provided.}
\pagebreak

\input{content/worktech}
\newpage

\section{Technical Results:}


\subsection{Performance tests and quality measures}
In this section, we will present the performance tests and quality measures applied to the codebase and stateless worker in runtime. These tests are essential to ensure that the application meets the expected performance standards and adheres to best practices in software development.

\subsubsection{Load Testing}

Load testing is a critical aspect of performance evaluation, especially for applications that handle high transaction volumes. In this project, we conducted load tests to assess the application's ability to process transactions efficiently under varying loads. The tests were designed to simulate the expected transaction volume and measure key performance metrics such as CPU utilization, average processing time, and transactions processed per second.
The load testing was conducted using the xK6-kafka tool, which is specifically designed for load testing Apache Kafka applications. This tool allows us to simulate a large number of transactions and measure the performance of the application under stress. We chose xK6-kafka as an alternative to Apache JMeter due to its better support for Avro messages, which are used in our application.


\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{metrics/k6-kafka-load-test.png}
    \caption{Load Testing Setup with xK6-kafka}
    \label{fig:load-test-setup}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{metrics/k6-bilan.png}
    \caption{Load Testing Results}
    \label{fig:load-test-results}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{metrics/kafka-metrics.png}
    \caption{Load Testing Results - Detailed Metrics}
    \label{fig:load-test-detailed}
\end{figure}

\FloatBarrier 
The load testing setup involved configuring the xK6-kafka tool to connect to our Kafka cluster and simulate a high volume of transactions. The tool was configured to send messages in Avro format, which is the format used by our application for transaction data.

The results captured during the load test are presented into the following table:

\begin{table}[h]
\centering
\caption{Kafka Producer Load Test Performance Results}
\label{tab:load_test_results}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Performance Metric} & \textbf{Threshold} & \textbf{Achieved Result} & \textbf{Status} \\
\hline
CPU Utilization & avg $<$ 80\% & 54.60\% & \textcolor{green}{\checkmark} \\
\hline
Send Duration (P95) & P(95) $<$ 200ms & 29ms & \textcolor{green}{\checkmark} \\
\hline
Success Rate & rate $>$ 0.99 & 100.00\% & \textcolor{green}{\checkmark} \\
\hline
\multicolumn{4}{|l|}{\textbf{Additional Validation Metrics}} \\
\hline
Total Checks & - & 1044 & 103.29/s \\
\hline
Check Success Rate & - & 100.00\% & 1044/1044 \\
\hline
Message Processing & - & All batches & \textcolor{green}{\checkmark} \\
\hline
Batch Timing & $<$ 500ms & Under target & \textcolor{green}{\checkmark} \\
\hline
\end{tabular}
\end{table}



\subsection{Performance Analysis and Proof-of-Concept Validation}
The load test results demonstrate a \textbf{successful proof-of-concept implementation} with an achieved throughput of \textbf{1038.12 TPS}, representing \textbf{69.2\% of the target 1500 TPS goal}. While the absolute target was not reached, this performance baseline validates the fundamental architecture and identifies clear optimization pathways for production deployment.



\subsubsection{Proof-of-Concept Context and Limitations}
The current implementation operates with \textbf{multiple Kafka worker instances} deployed on a \textbf{single development server}, which inherently constrains the achievable throughput due to several infrastructure limitations:

\begin{itemize}
    \item \textbf{Resource Contention on Single Host}: Multiple worker instances (meta, context, ic, sf) compete for CPU, memory, and I/O resources on the same physical server, creating bottlenecks not present in distributed production environments.
    
    \item \textbf{Development Infrastructure Limitations}: Local server hardware specifications and network configuration do not reflect the high-performance, dedicated infrastructure available in production client environments.
    
    \item \textbf{Shared Resource Architecture}: All worker types sharing the same host creates resource contention and limits the ability to achieve optimal per-worker performance, unlike production deployments with dedicated resources per service.
    
    \item \textbf{Network Topology Constraints}: Local server deployment lacks the optimized network architecture, load balancing, and distributed infrastructure that production client environments provide.
\end{itemize}

In production client environments, each worker type would be deployed on dedicated, high-performance infrastructure with optimized resource allocation, enabling linear scaling and significantly higher throughput capacity.
\subsubsection{Positive Performance Indicators}
Despite not achieving the absolute TPS target, the results demonstrate several \textbf{critical success factors}:

\begin{itemize}
    \item \textbf{Perfect Reliability}: 100\% success rate with zero message loss validates robust error handling and transaction processing logic
    
    \item \textbf{Good Latency Profile}: P95 latency of 29ms (85\% better than 200ms threshold) indicates efficient message processing with substantial headroom
    
    \item \textbf{Efficient Resource Utilization}: 54.6\% CPU usage demonstrates the system operates well within capacity limits, suggesting potential for optimization rather than resource exhaustion
    
    \item \textbf{Stable Performance}: 100\% check success rate across all 1044 test iterations confirms consistent system behavior under sustained load
\end{itemize}




\textbf{Conclusion}: The proof-of-concept successfully validates the fundamental transaction processing pipeline architecture while identifying specific optimization vectors for achieving and exceeding production performance targets. The 1038.12 TPS baseline provides a solid foundation for confident production deployment with clear scaling strategies.

\subsubsection{Performance Optimization}
% Discuss optimizations made based on load testing results


\subsection{Deployment}
In this section, we will present the technical results of the project, focusing on the deployment of the application and the tools used to ensure its quality. 
The deployment process is crucial as it allows us to make the application available for use and to ensure that it meets the expected standards of quality and security.


\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{img/arch/p2s-workers-deployment.drawio.png}
    \caption{Deployment Architecture}
    \label{fig:deployment}
\end{figure}
  
This architectural deployment demonstrates a distributed microservices approach optimized for high-availability transaction processing within IBM Cloud's multi-zone infrastructure. The system leverages  availability zones to ensure fault tolerance and geographic distribution, the compute engines deployed as instances in all availability zones as independent worker nodes provisioned with sufficient resources  to handle intensive financial calculations. 
The Meta Worker serves as the intelligent transaction dispatcher, while the second zone hosts the Context Qualification and Interchange Calculation engines responsible for transaction evaluation and fee computation. The third zone contains the Scheme Fees Calculation engine alongside the primary P2S application components.
This zonal distribution strategy ensures that critical processing components remain operational even during zone-level failures, while the stateless nature of each worker enables horizontal scaling based on transaction volume demands. The integration with ROKS (Red Hat OpenShift) provides container orchestration capabilities, while the Event Streams Enterprise implementation facilitates reliable Kafka-based messaging between components. The hybrid cloud approach, incorporating AWS services for extended P2S functionality, demonstrates the system's ability to leverage best-of-breed services across multiple cloud providers while maintaining centralized control within the IBM Cloud VPC infrastructure.




\section{Conclusion}
With this final step, we have completed the implementation of the project by applying the analytical and conceptual studies presented in the second chapter. We have presented the tools used for its development as well as the various interfaces that were created.
\pagebreak
